{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "import config\n",
    "from encoder.generator_model import Generator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_FFHQ = 'https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ'\n",
    "\n",
    "tflib.init_tf()\n",
    "with dnnlib.util.open_url(URL_FFHQ, cache_dir=config.cache_dir) as f:\n",
    "    generator_network, discriminator_network, Gs_network = pickle.load(f)\n",
    "\n",
    "generator = Generator(Gs_network, batch_size=1, randomize_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(latent_vector):\n",
    "    latent_vector = latent_vector.reshape((1, 18, 512))\n",
    "    generator.set_dlatents(latent_vector)\n",
    "    img_array = generator.generate_images()[0]\n",
    "    img = PIL.Image.fromarray(img_array, 'RGB')\n",
    "    return img.resize((1024, 1024))\n",
    "\n",
    "def move_and_show(latent_vector, directions, coeffs, desc):\n",
    "    fig,ax = plt.subplots(1, len(coeffs), figsize=(15, 10), dpi=80)\n",
    "    captions = ['Less %s' %desc , 'Input image', 'More %s' %desc]\n",
    "    for i, coeff in enumerate(coeffs):\n",
    "        new_latent_vector = latent_vector.copy()\n",
    "        for direction in directions:\n",
    "            new_latent_vector[:8] = (new_latent_vector + coeff*direction)[:8]\n",
    "        ax[i].imshow(generate_image(new_latent_vector))\n",
    "        ax[i].set_title(captions[i])\n",
    "    [x.axis('off') for x in ax]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading already learned latent directions\n",
    "smile_direction = np.load('ffhq_dataset/latent_directions/smile.npy')\n",
    "gender_direction = np.load('ffhq_dataset/latent_directions/gender.npy')\n",
    "age_direction = np.load('ffhq_dataset/latent_directions/age.npy')\n",
    "# In general it's possible to find directions of almost any face attributes: position, hair style or color ... \n",
    "# Additional scripts for doing so will be realised soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemakeup_direction = np.load('facemakeup_direction.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnd = np.random.RandomState(829) # ,1723,\n",
    "#latents = rnd.randn(1, Gs_network.input_shape[1])\n",
    "#print(latents.shape)\n",
    "#dlatents = Gs_network.components.mapping.run(latents, None)\n",
    "#print(dlatents.shape)\n",
    "#src_images = Gs_network.components.synthesis.run(dlatents, randomize_noise=False)\n",
    "dlatents = np.load('latent_representations/steph_01.npy')\n",
    "move_and_show(dlatents, [smile_direction], [-1, 0, 0.3], \"smile\")\n",
    "move_and_show(dlatents, [facemakeup_direction], [-1, 0, 3], \"makeup\")\n",
    "move_and_show(dlatents, [gender_direction], [-0.5, 0, 1], \"gender\")\n",
    "move_and_show(dlatents, [age_direction], [-2, 0, 1.5], \"age\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "model = '/home/bademi/stylegan-encoder/AFFACT.prototxt';\n",
    "weights = 'AFFACT.caffemodelAFFACT.caffemodel';\n",
    "\n",
    "caffe.set_mode_gpu();\n",
    "caffe.set_device(0);\n",
    "\n",
    "net = caffe.Net('/home/bademi/stylegan-encoder/AFFACT.prototxt', '/home/bademi/stylegan-encoder/AFFACT-S.caffemodel.h5', caffe.TEST)\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "\n",
    "image = np.array(imread('girl.png'));\n",
    "im_input = image[np.newaxis, np.newaxis, :, :]\n",
    "\n",
    "net.blobs['data'].reshape(*im_input.shape)\n",
    "net.blobs['data'].data[...] = im_input\n",
    "\n",
    "output = net.forward()\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 128\n",
    "synthesis_kwargs = dict(output_transform=dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True), minibatch_size=8)\n",
    "\n",
    "all_latents = []\n",
    "all_dlatents = []\n",
    "all_results = []\n",
    "for seed in range(1):\n",
    "    # Generate batch_size images\n",
    "    latents = np.random.RandomState(seed).randn(batch_size, Gs_network.input_shape[1])\n",
    "    dlatents = Gs_network.components.mapping.run(latents, None)\n",
    "    #images = Gs_network.components.synthesis.run(dlatents, truncation_psi=1,randomize_noise=False,**synthesis_kwargs)\n",
    "    images = Gs_network.run(latents, None, **synthesis_kwargs) # [seed, y, x, rgb]\n",
    "    #fig,ax = plt.subplots(int(len(images)/2), 2, figsize=(15, 200), dpi=80)\n",
    "\n",
    "    results = []\n",
    "    for i,image in enumerate(images):\n",
    "        image_from_array = PIL.Image.fromarray(image, 'RGB')\n",
    "        scaled_down_image = image_from_array.resize((224, 224))\n",
    "        array_from_image = np.array(scaled_down_image)\n",
    "        transposed_image = array_from_image.transpose(2,0,1)\n",
    "        input_image = transposed_image[np.newaxis, :, :, :]\n",
    "        net.blobs['data'].data[...] = input_image\n",
    "        res = net.forward();\n",
    "        all_latents.append(latents[i])\n",
    "        all_dlatents.append(dlatents[i])\n",
    "        all_results.append(res['attributes'][0])\n",
    "        #ax[int(i/2)][i%2].imshow(image_from_array)\n",
    "    np.save('batch_%s.npy' % seed, np.array(results))\n",
    "    print(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array([all_latents, all_dlatents, all_results])\n",
    "np.save('test_training_data.npy', training_data, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_training_data = np.load('test_training_data.npy', allow_pickle=True)\n",
    "saved_training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import logistic\n",
    "\n",
    "for i in range(40):\n",
    "    print(np.sum(logistic.cdf(saved_training_data[2][:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res = net.forward({image});\n",
    "#prob = res{1};\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_training_data[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.blobs['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imread, imsave, imresize\n",
    "import PIL.Image\n",
    "\n",
    "PIL.Image.fromarray(imread('girl.png'), 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffe\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "import numpy as np\n",
    "\n",
    "model = '/home/bademi/stylegan-encoder/AFFACT.prototxt';\n",
    "weights = 'AFFACT.caffemodelAFFACT.caffemodel';\n",
    "caffe.set_mode_gpu();\n",
    "caffe.set_device(0);\n",
    "net = caffe.Net('/home/bademi/stylegan-encoder/AFFACT.prototxt', '/home/bademi/stylegan-encoder/AFFACT-S.caffemodel.h5', caffe.TEST)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = np.array(imread('girl.png'));\n",
    "image = image.transpose(2,0,1)\n",
    "im_input = image[np.newaxis, :, :, :]\n",
    "print(im_input.shape)\n",
    "net.blobs['data'].data[...] = im_input\n",
    "\n",
    "output = net.forward()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = '''5_o_Clock_Shadow\n",
    "Arched_Eyebrows\n",
    "Attractive\n",
    "Bags_Under_Eyes\n",
    "Bald\n",
    "Bangs\n",
    "Big_Lips\n",
    "Big_Nose\n",
    "Black_Hair\n",
    "Blond_Hair\n",
    "Blurry\n",
    "Brown_Hair\n",
    "Bushy_Eyebrows\n",
    "Chubby\n",
    "Double_Chin\n",
    "Eyeglasses\n",
    "Goatee\n",
    "Gray_Hair\n",
    "Heavy_Makeup\n",
    "High_Cheekbones\n",
    "Male\n",
    "Mouth_Slightly_Open\n",
    "Mustache\n",
    "Narrow_Eyes\n",
    "No_Beard\n",
    "Oval_Face\n",
    "Pale_Skin\n",
    "Pointy_Nose\n",
    "Receding_Hairline\n",
    "Rosy_Cheeks\n",
    "Sideburns\n",
    "Smiling\n",
    "Straight_Hair\n",
    "Wavy_Hair\n",
    "Wearing_Earrings\n",
    "Wearing_Hat\n",
    "Wearing_Lipstick\n",
    "Wearing_Necklace\n",
    "Wearing_Necktie\n",
    "Young'''.split('\\n')[21]\n",
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import logistic\n",
    "\n",
    "for a,b in zip(logistic.cdf(output['attributes'][0]), attributes):\n",
    "    print(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seed = 345\n",
    "synthesis_kwargs = dict(output_transform=dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True), minibatch_size=8)\n",
    "\n",
    "# Generate batch_size images\n",
    "latents = np.random.RandomState(seed).randn(batch_size, Gs_network.input_shape[1])\n",
    "dlatents = Gs_network.components.mapping.run(latents, None)\n",
    "#images = Gs_network.components.synthesis.run(dlatents, truncation_psi=1,randomize_noise=False,**synthesis_kwargs)\n",
    "images = Gs_network.run(latents, None, **synthesis_kwargs) # [seed, y, x, rgb]\n",
    "\n",
    "fig,ax = plt.subplots(int(len(images)/2), 2, figsize=(15, 200), dpi=80)\n",
    "\n",
    "results = []\n",
    "for i,image in enumerate(images):\n",
    "    image_from_array = PIL.Image.fromarray(image, 'RGB')\n",
    "    scaled_down_image = image_from_array.resize((224, 224))\n",
    "    array_from_image = np.array(scaled_down_image)\n",
    "    transposed_image = array_from_image.transpose(2,0,1)\n",
    "    input_image = transposed_image[np.newaxis, :, :, :]\n",
    "    net.blobs['data'].data[...] = input_image\n",
    "    res = net.forward();\n",
    "    results.append([latents[i], dlatents[i], res['attributes'][0]])\n",
    "    ax[int(i/2)][i%2].imshow(image_from_array)\n",
    "    print(res)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
